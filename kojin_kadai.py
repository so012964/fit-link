import os
import json
import re
from typing import Dict, Any, List, Tuple, Optional

import pandas as pd
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI
import fitz  # PyMuPDF


# ============================================================
# 0. ç’°å¢ƒè¨­å®š
# ============================================================

load_dotenv()

st.set_page_config(
    page_title="Fit Link",
    layout="wide",
)

# ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰èªè¨¼ï¼ˆSecretsã«è¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿ï¼‰
if "APP_PASSWORD" in st.secrets:
    password = st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", type="password")
    if password != st.secrets["APP_PASSWORD"]:
        st.warning("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’çŸ¥ã£ã¦ã„ã‚‹äººã ã‘ãŒåˆ©ç”¨ã§ãã¾ã™ã€‚")
        st.stop()

# ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®æ–‡å­—ã‚µã‚¤ã‚ºã‚’èª¿æ•´
st.markdown("""
<style>
    .stCaption {
        font-size: 1.0rem !important;
    }
</style>
""", unsafe_allow_html=True)

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


# ============================================================
# 1. ãƒ¢ãƒ‡ãƒ«æŒ‡å®š
# ============================================================

MODEL_DIAG = "gpt-5-mini"  # æŠ½å‡ºãƒ»åˆ¤å®šãƒ»è³ªå•ãƒ»å¯¾è©±
MODEL_WRITE = "gpt-5.2"    # æœ€çµ‚è¿½è¨˜æ–‡ï¼ˆæå‡ºç”¨ï¼‰


# ============================================================
# 2. å‡ºåŠ›åˆ¶ç´„
# ============================================================

MAX_REQUIREMENTS = 25
MAX_GAPS = 25
EVIDENCE_MAX_CHARS = 140
MAX_QUESTIONS_LACK = 10
MAX_QUESTIONS_UNKNOWN = 5
ADDENDUM_MAX_CHARS = 1200

MIN_INPUT_CHARS = 100  # å…¥åŠ›ã®æœ€ä½æ–‡å­—æ•°ï¼ˆè­¦å‘Šç”¨ï¼‰


# ============================================================
# 3. ä¾‹å¤–
# ============================================================

class OpenAIAppError(RuntimeError):
    pass


# ============================================================
# 4. æ¨è«–æ¼ã‚Œæ¤œçŸ¥
# ============================================================

LEAK_PATTERNS = [
    r"æ¨è«–", r"æ€è€ƒéç¨‹", r"chain[- ]?of[- ]?thought", r"step\s*by\s*step",
    r"ãŠãã‚‰ã", r"ãŸã¶ã‚“", r"ã¨æ€ã‚ã‚Œ", r"ã¨è€ƒãˆ", r"è€ƒãˆã‚‹ã¨",
    r"ã—ãŸãŒã£ã¦", r"ã‚ˆã£ã¦", r"çµè«–",
    r"ç†ç”±\s*[:ï¼š]", r"æ ¹æ‹ \s*[:ï¼š]", r"åˆ¤æ–­\s*[:ï¼š]", r"åˆ†æ\s*[:ï¼š]",
]
LEAK_REGEX = re.compile("|".join(LEAK_PATTERNS), re.IGNORECASE)


def find_leak_paths_all(obj: Any, base_path: str = "$") -> List[Tuple[str, str]]:
    leaks: List[Tuple[str, str]] = []
    if isinstance(obj, dict):
        for k, v in obj.items():
            leaks.extend(find_leak_paths_all(v, f"{base_path}.{k}"))
    elif isinstance(obj, list):
        for i, v in enumerate(obj):
            leaks.extend(find_leak_paths_all(v, f"{base_path}[{i}]"))
    elif isinstance(obj, str):
        if LEAK_REGEX.search(obj):
            leaks.append((base_path, obj[:200].replace("\n", " ")))
    return leaks


def find_leak_paths_limited(obj: Any, allow_path_prefixes: List[str]) -> List[Tuple[str, str]]:
    leaks: List[Tuple[str, str]] = []

    def _is_relevant_for_traversal(path: str) -> bool:
        return any(path.startswith(pfx) or pfx.startswith(path) for pfx in allow_path_prefixes)

    def _walk(x: Any, path: str) -> None:
        if path != "$" and not _is_relevant_for_traversal(path):
            return
        if isinstance(x, dict):
            for k, v in x.items():
                _walk(v, f"{path}.{k}")
        elif isinstance(x, list):
            for i, v in enumerate(x):
                _walk(v, f"{path}[{i}]")
        elif isinstance(x, str):
            if any(path.startswith(pfx) for pfx in allow_path_prefixes):
                if LEAK_REGEX.search(x):
                    leaks.append((path, x[:200].replace("\n", " ")))

    _walk(obj, "$")
    return leaks


# ============================================================
# 5. JSON Schema
# ============================================================

SCHEMA_REQUIREMENTS: Dict[str, Any] = {
    "type": "object",
    "properties": {
        "requirements": {
            "type": "array",
            "maxItems": MAX_REQUIREMENTS,
            "items": {
                "type": "object",
                "properties": {
                    "id": {"type": "string", "maxLength": 10},
                    "text": {"type": "string", "maxLength": 240},
                },
                "required": ["id", "text"],
                "additionalProperties": False,
            },
        }
    },
    "required": ["requirements"],
    "additionalProperties": False,
}

SCHEMA_GAPS: Dict[str, Any] = {
    "type": "object",
    "properties": {
        "gaps": {
            "type": "array",
            "maxItems": MAX_GAPS,
            "items": {
                "type": "object",
                "properties": {
                    "requirement_id": {"type": "string", "maxLength": 10},
                    "status": {"type": "string", "enum": ["ã‚¢ãƒ”ãƒ¼ãƒ«æ¸ˆã¿", "è£œè¶³ãŒå¿…è¦", "è¨˜è¼‰ãªã—"]},
                    "resume_evidence": {"type": "string", "maxLength": EVIDENCE_MAX_CHARS},
                },
                "required": ["requirement_id", "status", "resume_evidence"],
                "additionalProperties": False,
            },
        }
    },
    "required": ["gaps"],
    "additionalProperties": False,
}

SCHEMA_QUESTIONS: Dict[str, Any] = {
    "type": "object",
    "properties": {
        "questions": {
            "type": "array",
            "maxItems": MAX_QUESTIONS_LACK + MAX_QUESTIONS_UNKNOWN,
            "items": {
                "type": "object",
                "properties": {
                    "requirement_id": {"type": "string", "maxLength": 10},
                    "question": {"type": "string", "maxLength": 220},
                },
                "required": ["requirement_id", "question"],
                "additionalProperties": False,
            },
        }
    },
    "required": ["questions"],
    "additionalProperties": False,
}

# å¯¾è©±ï¼ˆSTEP4ï¼‰å°‚ç”¨ã‚¹ã‚­ãƒ¼ãƒï¼šæ¬¡ã®1å• or ç´ æç¢ºå®šï¼ˆfinalizeï¼‰
SCHEMA_DIALOG: Dict[str, Any] = {
    "type": "object",
    "properties": {
        "facts": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "requirement_id": {"type": "string", "maxLength": 10},
                    "fact": {"type": "string", "maxLength": 240},
                },
                "required": ["requirement_id", "fact"],
                "additionalProperties": False,
            },
        },
        "next_question": {"type": "string", "maxLength": 240},
        "done": {"type": "boolean"},
    },
    "required": ["facts", "next_question", "done"],
    "additionalProperties": False,
}



# ============================================================
# 6. PDFâ†’ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
# ============================================================

@st.cache_data(show_spinner=False)
def extract_text_from_pdf(pdf_bytes: bytes, max_pages: int = 30) -> str:
    chunks: List[str] = []
    with fitz.open(stream=pdf_bytes, filetype="pdf") as doc:
        pages = min(len(doc), max_pages)
        for i in range(pages):
            chunks.append(doc[i].get_text("text"))
    return "\n".join(chunks).strip()


# ============================================================
# 7. Debug loggerï¼ˆä¸»ç”»é¢ã«ç”ŸJSONã¯å‡ºã•ãªã„ï¼‰
# ============================================================

DEBUG_MAX_EVENTS = 30
DEBUG_RAW_HEAD = 1200

def _safe_head(s: Optional[str], n: int = DEBUG_RAW_HEAD) -> str:
    if not isinstance(s, str):
        return ""
    s = s.replace("\r\n", "\n")
    return s[:n] + ("..." if len(s) > n else "")

def log_debug_event(event_type: str, payload: Dict[str, Any]) -> None:
    if "debug_events" not in st.session_state or st.session_state["debug_events"] is None:
        st.session_state["debug_events"] = []
    st.session_state["debug_events"].append({"type": event_type, "payload": payload})
    if len(st.session_state["debug_events"]) > DEBUG_MAX_EVENTS:
        st.session_state["debug_events"] = st.session_state["debug_events"][-DEBUG_MAX_EVENTS:]


# ============================================================
# 8. OpenAI å‘¼ã³å‡ºã—ï¼ˆschema validation + ä¾‹å¤–ã®è¦‹ã›æ–¹ï¼‰
# ============================================================

def _extract_refusal_or_raise(response: Any) -> None:
    try:
        if response.output and response.output[0].content:
            first = response.output[0].content[0]
            if isinstance(first, dict) and first.get("type") == "refusal":
                raise OpenAIAppError(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒæ‹’å¦ã•ã‚Œã¾ã—ãŸ: {first.get('refusal','')}")
            if getattr(first, "type", None) == "refusal":
                raise OpenAIAppError(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒæ‹’å¦ã•ã‚Œã¾ã—ãŸ: {getattr(first, 'refusal', '')}")
    except OpenAIAppError:
        raise
    except Exception:
        return


def get_output_text(response: Any) -> str:
    txt = getattr(response, "output_text", None)
    if isinstance(txt, str) and txt.strip():
        return txt.strip()

    outs: List[str] = []
    for item in (getattr(response, "output", None) or []):
        for c in (getattr(item, "content", None) or []):
            t = None
            if isinstance(c, dict):
                if c.get("type") in ("output_text", "text"):
                    t = c.get("text")
            else:
                if getattr(c, "type", None) in ("output_text", "text"):
                    t = getattr(c, "text", None)
            if isinstance(t, str) and t.strip():
                outs.append(t.strip())

    return "\n".join(outs).strip()


RE_REQUIREMENT_ID = re.compile(r"^R\d+$")

def validate_requirements_obj(data: Dict[str, Any]) -> None:
    reqs = data.get("requirements")
    if not isinstance(reqs, list):
        raise OpenAIAppError("requirements ãŒé…åˆ—ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

    ids: List[str] = []
    for r in reqs:
        if not isinstance(r, dict):
            raise OpenAIAppError("requirements ã®è¦ç´ å½¢å¼ãŒä¸æ­£ã§ã™ã€‚")
        rid = r.get("id")
        txt = r.get("text")
        if not isinstance(rid, str) or not RE_REQUIREMENT_ID.match(rid):
            raise OpenAIAppError("requirements ã®IDå½¢å¼ãŒä¸æ­£ã§ã™ï¼ˆR1, R2,...ï¼‰ã€‚")
        if not isinstance(txt, str) or not txt.strip():
            raise OpenAIAppError("requirements ã®textãŒç©ºã§ã™ã€‚")
        ids.append(rid)

    if len(ids) != len(set(ids)):
        raise OpenAIAppError("requirements ã®IDãŒé‡è¤‡ã—ã¦ã„ã¾ã™ã€‚")


def validate_gaps_obj(data: Dict[str, Any], requirements: Optional[Dict[str, Any]] = None) -> None:
    gaps = data.get("gaps")
    if not isinstance(gaps, list):
        raise OpenAIAppError("gaps ãŒé…åˆ—ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

    allowed = {"ã‚¢ãƒ”ãƒ¼ãƒ«æ¸ˆã¿", "è£œè¶³ãŒå¿…è¦", "è¨˜è¼‰ãªã—"}

    req_ids = None
    if isinstance(requirements, dict):
        reqs = requirements.get("requirements")
        if isinstance(reqs, list):
            req_ids = {r.get("id") for r in reqs if isinstance(r, dict)}

    for g in gaps:
        if not isinstance(g, dict):
            raise OpenAIAppError("gaps ã®è¦ç´ å½¢å¼ãŒä¸æ­£ã§ã™ã€‚")
        rid = g.get("requirement_id")
        status = g.get("status")
        ev = g.get("resume_evidence")

        if not isinstance(rid, str) or not rid:
            raise OpenAIAppError("gaps.requirement_id ãŒä¸æ­£ã§ã™ã€‚")
        if req_ids is not None and rid not in req_ids:
            raise OpenAIAppError("gaps.requirement_id ãŒ requirements ã¨ä¸€è‡´ã—ã¾ã›ã‚“ã€‚")

        if status not in allowed:
            raise OpenAIAppError("gaps.status ãŒä¸æ­£ã§ã™ã€‚")
        if not isinstance(ev, str):
            raise OpenAIAppError("gaps.resume_evidence ãŒä¸æ­£ã§ã™ã€‚")


def validate_questions_obj(data: Dict[str, Any], requirements: Optional[Dict[str, Any]] = None) -> None:
    qs = data.get("questions")
    if not isinstance(qs, list):
        raise OpenAIAppError("questions ãŒé…åˆ—ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

    req_ids = None
    if isinstance(requirements, dict):
        reqs = requirements.get("requirements")
        if isinstance(reqs, list):
            req_ids = {r.get("id") for r in reqs if isinstance(r, dict)}

    for q in qs:
        if not isinstance(q, dict):
            raise OpenAIAppError("questions ã®è¦ç´ å½¢å¼ãŒä¸æ­£ã§ã™ã€‚")
        rid = q.get("requirement_id")
        question = q.get("question")
        if not isinstance(rid, str) or not rid:
            raise OpenAIAppError("questions.requirement_id ãŒä¸æ­£ã§ã™ã€‚")
        if req_ids is not None and rid not in req_ids:
            raise OpenAIAppError("questions.requirement_id ãŒ requirements ã¨ä¸€è‡´ã—ã¾ã›ã‚“ã€‚")
        if not isinstance(question, str) or not question.strip():
            raise OpenAIAppError("questions.question ãŒç©ºã§ã™ã€‚")


def validate_dialog_obj(data: Dict[str, Any], requirements: Optional[Dict[str, Any]] = None) -> None:
    facts = data.get("facts")
    next_q = data.get("next_question")
    done = data.get("done")

    if not isinstance(facts, list):
        raise OpenAIAppError("dialog.facts ãŒé…åˆ—ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    if not isinstance(next_q, str):
        raise OpenAIAppError("dialog.next_question ãŒæ–‡å­—åˆ—ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    if not isinstance(done, bool):
        raise OpenAIAppError("dialog.done ãŒ boolean ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

    # factsè¦ç´ ãƒã‚§ãƒƒã‚¯
    req_ids = None
    if isinstance(requirements, dict):
        reqs = requirements.get("requirements")
        if isinstance(reqs, list):
            req_ids = {r.get("id") for r in reqs if isinstance(r, dict)}

    for f in facts:
        if not isinstance(f, dict):
            raise OpenAIAppError("dialog.facts ã®è¦ç´ å½¢å¼ãŒä¸æ­£ã§ã™ã€‚")
        rid = f.get("requirement_id")
        fact = f.get("fact")
        if not isinstance(rid, str) or not rid:
            raise OpenAIAppError("dialog.facts.requirement_id ãŒä¸æ­£ã§ã™ã€‚")
        if req_ids is not None and rid not in req_ids:
            raise OpenAIAppError("dialog.facts.requirement_id ãŒ requirements ã¨ä¸€è‡´ã—ã¾ã›ã‚“ã€‚")
        if not isinstance(fact, str) or not fact.strip():
            raise OpenAIAppError("dialog.facts.fact ãŒç©ºã§ã™ã€‚")



def call_openai_json_schema(
    *,
    model: str,
    system_prompt: str,
    user_prompt: str,
    schema_name: str,
    schema: Dict[str, Any],
    max_output_tokens: int,
    retries: int = 2,
    leak_check: bool = True,
    leak_allow_path_prefixes: Optional[List[str]] = None,
    context_requirements: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """
    - ä¸»ç”»é¢ã«ç”ŸJSONã‚’è¡¨ç¤ºã—ãªã„æ–¹é‡
    - å¤±æ•—æ™‚ã¯çŸ­ã„ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ä¾‹å¤–ã«ã¾ã¨ã‚ã€è©³ç´°ã¯ debug_events ã«æ ¼ç´
    """
    last_raw: Optional[str] = None
    last_leaks: List[Tuple[str, str]] = []

    for attempt in range(retries + 1):
        if attempt == 0:
            effective_user_prompt = user_prompt
        else:
            leak_lines = "\n".join([f"- {p}: {s}" for p, s in last_leaks[:10]])
            effective_user_prompt = f"""
å…ˆã»ã©ã®å‡ºåŠ›ã«ã€Œæ¨è«–éç¨‹ãƒ»ç†ç”±èª¬æ˜ã€ãŒæ··å…¥ã—ã¦ã„ã¾ã™ã€‚ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚

ç¦æ­¢:
- æ¨è«–/ç†ç”±/åˆ¤æ–­/åˆ†æ/çµè«– ãªã©ã®èª¬æ˜æ–‡
- ã€Œã€œã¨è€ƒãˆã¾ã™ã€ã€ŒãŠãã‚‰ãã€ãªã©ã®æ¨æ¸¬

æ··å…¥ç®‡æ‰€:
{leak_lines}

ã€ä¿®æ­£å¯¾è±¡ã€‘
{last_raw}

ã€å…ƒã®æŒ‡ç¤ºã€‘
{user_prompt}
""".strip()

        try:
            response = client.responses.create(
                model=model,
                input=[
                    {"role": "system", "content": system_prompt.strip()},
                    {"role": "user", "content": effective_user_prompt.strip()},
                ],
                max_output_tokens=max_output_tokens,
                text={
                    "format": {
                        "type": "json_schema",
                        "name": schema_name,
                        "schema": schema,
                        "strict": True,
                    }
                },
            )
        except Exception as e:
            log_debug_event("api_call_error", {
                "schema_name": schema_name, "attempt": attempt, "model": model, "error": repr(e),
            })
            raise OpenAIAppError("APIå‘¼ã³å‡ºã—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚") from e

        try:
            _extract_refusal_or_raise(response)
        except OpenAIAppError as e:
            log_debug_event("refusal", {
                "schema_name": schema_name, "attempt": attempt, "model": model, "error": str(e),
            })
            raise

        if response.status == "incomplete":
            reason = getattr(getattr(response, "incomplete_details", None), "reason", None)
            log_debug_event("incomplete", {
                "schema_name": schema_name, "attempt": attempt, "model": model, "reason": reason,
            })
            raise OpenAIAppError("å‡ºåŠ›ãŒä¸å®Œå…¨ã§ã—ãŸã€‚å…¥åŠ›ã‚’çŸ­ãã™ã‚‹ã‹ã€ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")

        if response.status != "completed":
            log_debug_event("unexpected_status", {
                "schema_name": schema_name, "attempt": attempt, "model": model, "status": response.status,
            })
            raise OpenAIAppError("å‡¦ç†ãŒå®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")

        raw = get_output_text(response)
        last_raw = raw

        if not raw:
            log_debug_event("empty_output", {
                "schema_name": schema_name, "attempt": attempt, "model": model,
            })
            raise OpenAIAppError("å‡ºåŠ›ãŒç©ºã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")

        try:
            data = json.loads(raw)
        except json.JSONDecodeError as e:
            log_debug_event("json_decode_error", {
                "schema_name": schema_name, "attempt": attempt, "model": model,
                "error": repr(e), "raw_head": _safe_head(raw),
            })
            raise OpenAIAppError("å‡ºåŠ›å½¢å¼ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚") from e

        if leak_check:
            if leak_allow_path_prefixes:
                leaks = find_leak_paths_limited(data, leak_allow_path_prefixes)
            else:
                leaks = find_leak_paths_all(data)

            if leaks:
                last_leaks = leaks
                log_debug_event("leak_detected", {
                    "schema_name": schema_name, "attempt": attempt, "model": model,
                    "leaks": leaks[:10], "raw_head": _safe_head(raw),
                })
                continue

        # è»½é‡ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆstrict schemaã®ä¿é™ºï¼‰
        try:
            if schema_name == "requirements_schema":
                validate_requirements_obj(data)
            elif schema_name == "gaps_schema":
                validate_gaps_obj(data, requirements=context_requirements)
            elif schema_name == "questions_schema":
                validate_questions_obj(data, requirements=context_requirements)
            elif schema_name == "dialog_schema":
                validate_dialog_obj(data)
        except OpenAIAppError as ve:
            log_debug_event("validation_error", {
                "schema_name": schema_name, "attempt": attempt, "model": model,
                "error": str(ve), "raw_head": _safe_head(raw),
            })
            raise OpenAIAppError("å‡ºåŠ›å†…å®¹ã®æ•´åˆãƒã‚§ãƒƒã‚¯ã§ã‚¨ãƒ©ãƒ¼ã«ãªã‚Šã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚") from ve

        return data

    log_debug_event("retries_exhausted", {
        "schema_name": schema_name,
        "model": model,
        "last_raw_head": _safe_head(last_raw),
        "last_leaks": last_leaks[:10],
    })
    raise OpenAIAppError("å‡ºåŠ›ã®æ•´å½¢ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")


def call_openai_text(
    *,
    model: str,
    system_prompt: str,
    user_prompt: str,
    max_output_tokens: int,
) -> str:
    try:
        response = client.responses.create(
            model=model,
            input=[
                {"role": "system", "content": system_prompt.strip()},
                {"role": "user", "content": user_prompt.strip()},
            ],
            max_output_tokens=max_output_tokens,
        )
    except Exception as e:
        log_debug_event("api_call_error_text", {"model": model, "error": repr(e)})
        raise OpenAIAppError("æ–‡ç« ç”Ÿæˆã®å‘¼ã³å‡ºã—ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚") from e

    _extract_refusal_or_raise(response)

    if response.status == "incomplete":
        reason = getattr(getattr(response, "incomplete_details", None), "reason", None)
        log_debug_event("incomplete_text", {"model": model, "reason": reason})
        raise OpenAIAppError("æ–‡ç« ç”ŸæˆãŒä¸å®Œå…¨ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")

    if response.status != "completed":
        log_debug_event("unexpected_status_text", {"model": model, "status": response.status})
        raise OpenAIAppError("æ–‡ç« ç”ŸæˆãŒå®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")

    out = get_output_text(response)
    if not out:
        log_debug_event("empty_output_text", {"model": model})
        raise OpenAIAppError("æ–‡ç« ç”Ÿæˆã®å‡ºåŠ›ãŒç©ºã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")

    return out


# ============================================================
# 9. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
# ============================================================

SYSTEM_REQUIREMENTS = """
ã‚ãªãŸã¯æ±‚äººç¥¨ã‹ã‚‰ã€Œå¿œå‹Ÿè€…ã«æ±‚ã‚ã‚‰ã‚Œã‚‹ã‚¹ã‚­ãƒ«ãƒ»çµŒé¨“ã€ã‚’æŠ½å‡ºã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚

é‡è¦ãªåŒºåˆ¥:
- ã€Œå…¥ç¤¾å¾Œã«æ‹…å½“ã™ã‚‹æ¥­å‹™å†…å®¹ã€ã¯æŠ½å‡ºå¯¾è±¡å¤–
- ã€Œå¿œå‹Ÿæ™‚ç‚¹ã§æŒã£ã¦ã„ã¦ã»ã—ã„ã‚¹ã‚­ãƒ«ãƒ»çµŒé¨“ãƒ»è³‡æ ¼ã€ã®ã¿ã‚’æŠ½å‡º

ä¾‹:
- âŒã€Œã€‡ã€‡å¤§å­¦ã§IRæˆ¦ç•¥ã‚’ç­–å®šã™ã‚‹ã€â†’ ã“ã‚Œã¯å…¥ç¤¾å¾Œã®æ¥­å‹™å†…å®¹ãªã®ã§é™¤å¤–
- âœ…ã€ŒIRæˆ¦ç•¥ç­–å®šã®çµŒé¨“ã€â†’ ã“ã‚Œã¯æ±‚ã‚ã‚‰ã‚Œã‚‹ã‚¹ã‚­ãƒ«
- âŒã€Œâ–³â–³ã‚·ã‚¹ãƒ†ãƒ ã‚’å°å…¥ãƒ»é‹ç”¨ã™ã‚‹ã€â†’ ã“ã‚Œã¯å…¥ç¤¾å¾Œã®æ¥­å‹™å†…å®¹ãªã®ã§é™¤å¤–
- âœ…ã€Œã‚·ã‚¹ãƒ†ãƒ å°å…¥ãƒ»é‹ç”¨ã®çµŒé¨“ã€â†’ ã“ã‚Œã¯æ±‚ã‚ã‚‰ã‚Œã‚‹ã‚¹ã‚­ãƒ«

å³å®ˆ:
- ç‰¹å®šã®çµ„ç¹”åï¼ˆå¿œå‹Ÿå…ˆä¼æ¥­ãƒ»å¤§å­¦åï¼‰ã§ã®çµŒé¨“ã¯æ±‚ã‚ãªã„ï¼ˆæ±ç”¨çš„ãªã‚¹ã‚­ãƒ«ã¨ã—ã¦æŠ½å‡ºï¼‰
- æ¨è«–ãƒ»ç†ç”±èª¬æ˜ã¯å‡ºåŠ›ã—ãªã„
- JSONã®ã¿å‡ºåŠ›
""".strip()

SYSTEM_GAP = """
ã‚ãªãŸã¯æ±‚äººè¦ä»¶ã¨è·å‹™çµŒæ­´æ›¸ã®ä¸€è‡´çŠ¶æ³ã‚’åˆ¤å®šã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚

å³å®ˆ:
- status ã¯ã€Œã‚¢ãƒ”ãƒ¼ãƒ«æ¸ˆã¿ã€ã€Œè£œè¶³ãŒå¿…è¦ã€ã€Œè¨˜è¼‰ãªã—ã€ã®ã„ãšã‚Œã‹
- resume_evidence ã¯è·å‹™çµŒæ­´æ›¸ã‹ã‚‰ã®çŸ­ã„å¼•ç”¨ã¾ãŸã¯è¦ç´„ã®ã¿
- æ¨è«–ãƒ»ç†ç”±èª¬æ˜ã¯å‡ºåŠ›ã—ãªã„
- JSONã®ã¿å‡ºåŠ›
""".strip()

SYSTEM_INTERVIEW = """
ã‚ãªãŸã¯è·å‹™çµŒæ­´æ›¸ã®è£œå¼·ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
å¿œå‹Ÿè€…ãŒã€Œæ›¸ãæ¼ã‚‰ã—ã¦ã„ã‚‹çµŒé¨“ã‚„å®Ÿç¸¾ã€ã‚’æ€ã„å‡ºã›ã‚‹ã‚ˆã†ã€è³ªå•ã‚’ä½œæˆã—ã¾ã™ã€‚

å³å®ˆ:
- è³ªå•ã¯1é …ç›®ã«ã¤ãæœ€å¤§1å•
- å°‹å•èª¿ã§ã¯ãªãã€ã‚µãƒãƒ¼ãƒˆèª¿ã§
- æ¨è«–ãƒ»ç†ç”±èª¬æ˜ã¯å‡ºåŠ›ã—ãªã„
- JSONã®ã¿å‡ºåŠ›
""".strip()

# STEP4ï¼šå¯¾è©±å°‚ç”¨ãƒ­ãƒ¼ãƒ«
SYSTEM_DIALOG_EDITOR = """
ã‚ãªãŸã¯è·å‹™çµŒæ­´æ›¸ã®è£œå¼·ã‚’æ”¯æ´ã™ã‚‹ç·¨é›†è€…ã§ã™ã€‚
æ±‚äººè¦ä»¶ï¼ˆ1ä»¶ï¼‰ã«å¯¾ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ›¸ãè¶³ã›ã‚‹ã€Œäº‹å®Ÿã€ã‚’æ€ã„å‡ºã›ã‚‹ã‚ˆã†ã«çŸ­ã„è³ªå•ã‚’ã—ã¾ã™ã€‚

å³å®ˆ:
- æ¨æ¸¬ãƒ»èª‡å¼µã¯ç¦æ­¢
- 1å›ã®å‡ºåŠ›ã¯ã€Œæ¬¡ã®1å•ã€ã‹ã€Œç¢ºå®šã—ãŸå†…å®¹ï¼ˆç®‡æ¡æ›¸ãï¼‰ã€ã®ã©ã¡ã‚‰ã‹
- è³ªå•ã™ã‚‹ã¨ãï¼šè³ªå•ã¯1ã¤ã ã‘ã€çŸ­ãå…·ä½“çš„ã«
- ç¢ºå®šã™ã‚‹ã¨ãï¼šfacts ã«æ›¸ãè¶³ã™å†…å®¹ï¼ˆç®‡æ¡æ›¸ãã€äº‹å®Ÿã®ã¿ï¼‰ã‚’å…¥ã‚Œã‚‹ï¼ˆæ•°å­—ãƒ»æœŸé–“ãƒ»å½¹å‰²ãƒ»æˆæœãŒã‚ã‚‹ã¨è‰¯ã„ï¼‰
- æ¨è«–ãƒ»ç†ç”±èª¬æ˜ã¯å‡ºåŠ›ã—ãªã„
- JSONã®ã¿å‡ºåŠ›
""".strip()

SYSTEM_WRITER = """
ã‚ãªãŸã¯æå‡ºç”¨ã®è·å‹™çµŒæ­´æ›¸ã«æ›¸ãè¶³ã™æ–‡ç« ã‚’ä½œæˆã™ã‚‹ç·¨é›†è€…ã§ã™ã€‚

ã‚ãªãŸã®å½¹å‰²:
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ã—ãŸç´ æï¼ˆäº‹å®Ÿï¼‰ã‚’ã‚‚ã¨ã«ã€è·å‹™çµŒæ­´æ›¸ã«ãµã•ã‚ã—ã„æ–‡ç« ã«ä»•ä¸Šã’ã‚‹
- èª¤å­—è„±å­—ã‚’ä¿®æ­£ã™ã‚‹
- æ–‡ç« ã¨ã—ã¦è‡ªç„¶ãªè¡¨ç¾ãƒ»ä½“è£ã«æ•´ãˆã‚‹
- ç®‡æ¡æ›¸ãã®å ´åˆã¯ã€è¡¨ç¾ã‚’çµ±ä¸€ã—ã€èª­ã¿ã‚„ã™ãæ•´å½¢ã™ã‚‹

å³å®ˆ:
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå›ç­”ã—ãŸäº‹å®Ÿã®æ„å‘³ã‚’å¤‰ãˆãªã„
- æ¨æ¸¬ãƒ»èª‡å¼µã¯ç¦æ­¢ï¼ˆäº‹å®Ÿã‚’è†¨ã‚‰ã¾ã›ãªã„ï¼‰
- ä¸æ˜ç‚¹ã¯ï¼ˆè¦ç¢ºèªï¼šxxxï¼‰ã§æ®‹ã™
""".strip()


# ============================================================
# 10. ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ============================================================

NON_ESSENTIAL_PATTERNS = [
    r"æå‡º", r"å¿œå‹Ÿ", r"ç”³è¾¼", r"ç”³è«‹", r"ã‚¨ãƒ³ãƒˆãƒª",
    r"ç· åˆ‡", r"ã€†åˆ‡", r"æœŸé™",
    r"éƒµé€", r"ãƒ¡ãƒ¼ãƒ«", r"ãƒ•ã‚©ãƒ¼ãƒ ", r"URL", r"Web", r"ã‚ªãƒ³ãƒ©ã‚¤ãƒ³",
    r"å±¥æ­´æ›¸", r"è·å‹™çµŒæ­´æ›¸", r"æ·»ä»˜", r"PDF", r"æ›¸é¡",
]
NON_ESSENTIAL_REGEX = re.compile("|".join(NON_ESSENTIAL_PATTERNS), re.IGNORECASE)

def default_include_flag(text: str) -> bool:
    if not isinstance(text, str):
        return True
    return not bool(NON_ESSENTIAL_REGEX.search(text))

def build_selected_requirements_from_editor(edited_df: pd.DataFrame) -> Dict[str, Any]:
    selected = edited_df[edited_df["è¨ºæ–­ã«å«ã‚ã‚‹"] == True].copy()  # noqa: E712
    return {"requirements": [{"id": r["ID"], "text": r["å†…å®¹"]} for _, r in selected.iterrows()]}

def uniq_questions_by_requirement_id(questions_obj: Dict[str, Any]) -> Dict[str, Any]:
    seen = set()
    uniq: List[Dict[str, Any]] = []
    for q in (questions_obj.get("questions", []) or []):
        rid = q.get("requirement_id")
        if not rid or rid in seen:
            continue
        seen.add(rid)
        uniq.append(q)
    return {"questions": uniq}

def build_need_review_requirement_ids(gaps: Dict[str, Any]) -> List[str]:
    """
    å¯¾è©±å¯¾è±¡ï¼ˆè£œè¶³ãŒå¿…è¦/è¨˜è¼‰ãªã—ï¼‰ã® requirement_id ã ã‘æŠ½å‡ºã€‚
    """
    ids: List[str] = []
    for g in (gaps.get("gaps", []) or []):
        if g.get("status") in ("è£œè¶³ãŒå¿…è¦", "è¨˜è¼‰ãªã—"):
            rid = g.get("requirement_id")
            if isinstance(rid, str) and rid:
                ids.append(rid)

    # é‡è¤‡æ’é™¤ï¼ˆé †åºç¶­æŒï¼‰
    seen = set()
    uniq = []
    for rid in ids:
        if rid in seen:
            continue
        seen.add(rid)
        uniq.append(rid)
    return uniq


# ============================================================
# 11. è¨ºæ–­ãƒ­ã‚¸ãƒƒã‚¯
# ============================================================

def extract_requirements(job_text: str) -> Dict[str, Any]:
    return call_openai_json_schema(
        model=MODEL_DIAG,
        system_prompt=SYSTEM_REQUIREMENTS,
        user_prompt=f"""
æ±‚äººç¥¨ã‹ã‚‰ã€Œæ±‚ã‚ã‚‰ã‚Œã‚‹ã‚¹ã‚­ãƒ«ãƒ»çµŒé¨“ã€ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ã€æ±‚äººç¥¨ã€‘
{job_text}

åˆ¶ç´„:
- é‡è¦åº¦é †ã«æœ€å¤§{MAX_REQUIREMENTS}ä»¶ã¾ã§
- é‡è¤‡ã¯çµ±åˆ
- IDã¯ R1, R2, ... ã¨é€£ç•ª
- JSONã®ã¿
""".strip(),
        schema_name="requirements_schema",
        schema=SCHEMA_REQUIREMENTS,
        max_output_tokens=4096,
        retries=2,
        leak_check=False,
    )

def classify_gaps(requirements: Dict[str, Any], resume_text: str) -> Dict[str, Any]:
    return call_openai_json_schema(
        model=MODEL_DIAG,
        system_prompt=SYSTEM_GAP,
        user_prompt=f"""
ä»¥ä¸‹ã®ã€Œæ±‚ã‚ã‚‰ã‚Œã‚‹ã‚¹ã‚­ãƒ«ãƒ»çµŒé¨“ã€ã¨è·å‹™çµŒæ­´æ›¸ã‚’ç…§åˆã—ã¦ãã ã•ã„ã€‚

ã€æ±‚ã‚ã‚‰ã‚Œã‚‹ã‚¹ã‚­ãƒ«ãƒ»çµŒé¨“ï¼ˆJSONï¼‰ã€‘
{json.dumps(requirements, ensure_ascii=False)}

ã€è·å‹™çµŒæ­´æ›¸ã€‘
{resume_text}

åˆ¶ç´„:
- status ã¯ã€Œã‚¢ãƒ”ãƒ¼ãƒ«æ¸ˆã¿ã€ã€Œè£œè¶³ãŒå¿…è¦ã€ã€Œè¨˜è¼‰ãªã—ã€ã®ã„ãšã‚Œã‹
- resume_evidence ã¯æœ€å¤§{EVIDENCE_MAX_CHARS}æ–‡å­—
- æ ¹æ‹ ãŒç„¡ã„å ´åˆã¯ "è©²å½“ã™ã‚‹è¨˜è¼‰ãªã—" ã¨å…¥ã‚Œã‚‹
- JSONã®ã¿

åˆ¤å®šåŸºæº–:
- ã‚¢ãƒ”ãƒ¼ãƒ«æ¸ˆã¿ = è·å‹™çµŒæ­´æ›¸ã«ååˆ†ãªè¨˜è¼‰ãŒã‚ã‚‹
- è£œè¶³ãŒå¿…è¦ = è¨˜è¼‰ã¯ã‚ã‚‹ãŒæƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹
- è¨˜è¼‰ãªã— = é–¢é€£ã™ã‚‹è¨˜è¼‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„
""".strip(),
        schema_name="gaps_schema",
        schema=SCHEMA_GAPS,
        max_output_tokens=8192,
        retries=2,
        leak_check=True,
        leak_allow_path_prefixes=["$.gaps"],
        context_requirements=requirements,
    )

def make_questions(gaps: Dict[str, Any], requirements: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    return call_openai_json_schema(
        model=MODEL_DIAG,
        system_prompt=SYSTEM_INTERVIEW,
        user_prompt=f"""
ä»¥ä¸‹ã®ã€Œè£œè¶³ãŒå¿…è¦ã€ã€Œè¨˜è¼‰ãªã—ã€ã®é …ç›®ã«ã¤ã„ã¦ã€æ€ã„å‡ºã—ã‚’ä¿ƒã™è³ªå•ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€ç…§åˆçµæœï¼ˆJSONï¼‰ã€‘
{json.dumps(gaps, ensure_ascii=False)}

ç›®çš„:
- å¿œå‹Ÿè€…ãŒã€Œæ›¸ãæ¼ã‚‰ã—ã¦ã„ã‚‹çµŒé¨“ã‚„å®Ÿç¸¾ã€ã‚’æ€ã„å‡ºã›ã‚‹ã‚ˆã†ã«ã™ã‚‹
- è·å‹™çµŒæ­´æ›¸ã«æ›¸ãè¶³ã›ã‚‹å†…å®¹ã‚’å¼•ãå‡ºã™

åˆ¶ç´„:
- ã€Œè£œè¶³ãŒå¿…è¦ã€ã¯æœ€å¤§{MAX_QUESTIONS_LACK}å•
- ã€Œè¨˜è¼‰ãªã—ã€ã¯æœ€å¤§{MAX_QUESTIONS_UNKNOWN}å•
- 1é …ç›®ã«ã¤ãæœ€å¤§1å•
- æŒ‡ç¤ºèªï¼ˆã“ã®æ¥­å‹™/ã“ã®çµŒé¨“ ç­‰ï¼‰ã‚’é¿ã‘ã‚‹
- JSONã®ã¿
""".strip(),
        schema_name="questions_schema",
        schema=SCHEMA_QUESTIONS,
        max_output_tokens=4096,
        retries=2,
        leak_check=True,
        leak_allow_path_prefixes=["$.questions"],
        context_requirements=requirements,
    )

def dialog_refine_one_requirement(
    *,
    requirement_id: str,
    requirement_text: str,
    resume_text: str,
    chat_history: List[Dict[str, str]],
) -> Dict[str, Any]:
    """
    STEP4ï¼š1è¦ä»¶ã«ã¤ã„ã¦å¯¾è©±ã‚’1ã‚¹ãƒ†ãƒƒãƒ—é€²ã‚ã‚‹
    chat_history: [{"role":"user"/"assistant", "content":"..."}]
    """
    user_prompt = f"""
å¯¾è±¡è¦ä»¶:
- id: {requirement_id}
- text: {requirement_text}

è·å‹™çµŒæ­´æ›¸ï¼ˆå‚è€ƒï¼‰:
{resume_text}

ã“ã‚Œã¾ã§ã®å¯¾è©±:
{json.dumps(chat_history, ensure_ascii=False)}

ä¸Šã®æƒ…å ±ã‚’è¸ã¾ãˆã€æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸ã‚“ã§ãã ã•ã„:
- ã¾ã æƒ…å ±ãŒè¶³ã‚Šãªã„ãªã‚‰ done=false ã§æ¬¡ã®1å•ã‚’ next_question ã«å…¥ã‚Œã‚‹
- ååˆ†ãªã‚‰ done=true ã§ factsï¼ˆæ›¸ãè¶³ã™å†…å®¹ï¼‰ã‚’ç¢ºå®šã™ã‚‹
""".strip()

    return call_openai_json_schema(
        model=MODEL_DIAG,
        system_prompt=SYSTEM_DIALOG_EDITOR,
        user_prompt=user_prompt,
        schema_name="dialog_schema",
        schema=SCHEMA_DIALOG,
        max_output_tokens=2048,
        retries=1,
        leak_check=True,
        leak_allow_path_prefixes=["$"],
    )

def write_addendum(resume_text: str, requirements: Dict[str, Any], answers: Dict[str, str]) -> str:
    return call_openai_text(
        model=MODEL_WRITE,
        system_prompt=SYSTEM_WRITER,
        user_prompt=f"""
ä»¥ä¸‹ã‚’è¸ã¾ãˆã¦ã€è·å‹™çµŒæ­´æ›¸ã«æ›¸ãè¶³ã™æ–‡ç« ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€å¯¾è±¡ã®æ±‚ã‚ã‚‰ã‚Œã‚‹ã‚¹ã‚­ãƒ«ãƒ»çµŒé¨“ï¼ˆJSONï¼‰ã€‘
{json.dumps(requirements, ensure_ascii=False)}

ã€å…ƒã®è·å‹™çµŒæ­´æ›¸ã€‘
{resume_text}

ã€æ›¸ãè¶³ã™å†…å®¹ï¼ˆJSONï¼‰ã€‘
{json.dumps(answers, ensure_ascii=False)}

åˆ¶ç´„:
- æœ€å¤§{ADDENDUM_MAX_CHARS}æ–‡å­—ã‚’ç›®å®‰ã«
- å›ç­”ã«ãªã„äº‹å®Ÿã¯è¿½åŠ ã—ãªã„
- ä¸æ˜ç‚¹ã¯ï¼ˆè¦ç¢ºèªï¼šxxxï¼‰ã§æ®‹ã™
- å‡ºåŠ›ã¯æ›¸ãè¶³ã™æ–‡ç« ã®ã¿

æ¨å¥¨æ§‹æˆ:
- æ›¸ãè¶³ã—å€™è£œï¼ˆç®‡æ¡æ›¸ã 3ã€œ8ç‚¹ï¼‰
- å¿…è¦ãªã‚‰çŸ­ã„è£œè¶³æ–‡
""".strip(),
        max_output_tokens=8192,
    )


# ============================================================
# 12. UIãƒ˜ãƒ«ãƒ‘ãƒ¼
# ============================================================

def render_progress_stepper(current_step: int):
    steps = [
        "â‘  å…¥åŠ›",
        "â‘¡ ã‚¹ã‚­ãƒ«æ•´ç†",
        "â‘¢ çµŒæ­´ã¨æ¯”è¼ƒ",
        "â‘£ å†…å®¹ã‚’æ•´ç†",
        "â‘¤ æ–‡ç« ã‚’ä½œæˆ",
    ]
    cols = st.columns(len(steps))
    for i, (col, step_name) in enumerate(zip(cols, steps)):
        step_num = i + 1
        with col:
            if step_num < current_step:
                st.markdown(
                    f"<div style='text-align:center; padding:10px; background-color:#d4edda; "
                    f"border-radius:8px; border:2px solid #28a745;'><b>âœ… {step_name}</b></div>",
                    unsafe_allow_html=True
                )
            elif step_num == current_step:
                st.markdown(
                    f"<div style='text-align:center; padding:10px; background-color:#cce5ff; "
                    f"border-radius:8px; border:2px solid #007bff;'><b>â–¶ {step_name}</b></div>",
                    unsafe_allow_html=True
                )
            else:
                st.markdown(
                    f"<div style='text-align:center; padding:10px; background-color:#f8f9fa; "
                    f"border-radius:8px; border:1px solid #dee2e6; color:#6c757d;'>{step_name}</div>",
                    unsafe_allow_html=True
                )

def get_status_style(status: str) -> Tuple[str, str]:
    if status == "ã‚¢ãƒ”ãƒ¼ãƒ«æ¸ˆã¿":
        return "#d4edda", "âœ…"
    elif status == "è£œè¶³ãŒå¿…è¦":
        return "#fff3cd", "âš ï¸"
    elif status == "è¨˜è¼‰ãªã—":
        return "#f8d7da", "âŒ"
    return "#ffffff", ""

def render_gaps_summary(gaps: Dict[str, Any]):
    gap_list = gaps.get("gaps", [])
    counts = {"ã‚¢ãƒ”ãƒ¼ãƒ«æ¸ˆã¿": 0, "è£œè¶³ãŒå¿…è¦": 0, "è¨˜è¼‰ãªã—": 0}
    for g in gap_list:
        s = g.get("status", "")
        if s in counts:
            counts[s] += 1

    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("âœ… ã‚¢ãƒ”ãƒ¼ãƒ«æ¸ˆã¿", counts["ã‚¢ãƒ”ãƒ¼ãƒ«æ¸ˆã¿"])
    with col2:
        st.metric("âš ï¸ è£œè¶³ãŒå¿…è¦", counts["è£œè¶³ãŒå¿…è¦"])
    with col3:
        st.metric("âŒ è¨˜è¼‰ãªã—", counts["è¨˜è¼‰ãªã—"])

def render_gaps_detail(gaps: Dict[str, Any], requirements: Dict[str, Any]):
    gap_list = gaps.get("gaps", [])
    req_map = {r["id"]: r["text"] for r in requirements.get("requirements", [])}

    for g in gap_list:
        rid = g.get("requirement_id", "")
        status = g.get("status", "")
        evidence = g.get("resume_evidence", "")
        req_text = req_map.get(rid, "ï¼ˆä¸æ˜ï¼‰")

        bg_color, emoji = get_status_style(status)
        st.markdown(f"""
        <div style="background-color:{bg_color}; padding:12px; border-radius:8px; margin-bottom:8px;">
            <div style="display:flex; justify-content:space-between; align-items:center;">
                <span><b>{rid}</b>: {req_text[:60]}{'...' if len(req_text) > 60 else ''}</span>
                <span style="font-size:1.1em;">{emoji} {status}</span>
            </div>
            <div style="margin-top:6px; color:#555; font-size:0.9em;">
                æ ¹æ‹ : {evidence if evidence else 'â€”'}
            </div>
        </div>
        """, unsafe_allow_html=True)

def render_questions_cards(questions: Dict[str, Any], requirements: Dict[str, Any]):
    q_list = questions.get("questions", [])
    req_map = {r["id"]: r["text"] for r in requirements.get("requirements", [])}

    for q in q_list:
        rid = q.get("requirement_id", "")
        question = q.get("question", "")
        req_text = req_map.get(rid, "")
        st.markdown(f"""
        <div style="background-color:#e7f3ff; padding:12px; border-radius:8px;
                    margin-bottom:10px; border-left:4px solid #007bff;">
            <div style="font-size:0.85em; color:#666; margin-bottom:6px;">
                ğŸ“Œ {rid}: {req_text[:60]}{'...' if len(req_text) > 60 else ''}
            </div>
            <div style="font-size:1.0em;">
                ğŸ’¬ {question}
            </div>
        </div>
        """, unsafe_allow_html=True)

def render_requirement_card(rid: str, req_text: str, subtitle: str = ""):
    sub = f"<div style='font-size:0.85em; color:#666; margin-bottom:8px;'>{subtitle}</div>" if subtitle else ""
    st.markdown(f"""
    <div style="background-color:#f8f9fa; padding:16px; border-radius:8px; margin-bottom:8px; border-left:4px solid #007bff;">
        {sub}
        <div style="font-weight:600; margin-bottom:8px;">ğŸ“Œ æ±‚ã‚ã‚‰ã‚Œã‚‹ã‚¹ã‚­ãƒ«ï¼ˆ{rid}ï¼‰</div>
        <div style="color:#333;">{req_text}</div>
    </div>
    """, unsafe_allow_html=True)


# ============================================================
# 13. ãƒ¡ã‚¤ãƒ³UIï¼ˆçŠ¶æ…‹ç®¡ç†ï¼‰
# ============================================================

st.title("Fit Link")
st.markdown("""
<p style="font-size: 1.1rem; color: #666; margin-top: -10px;">
â€”æ±‚ã‚ã‚‰ã‚Œã¦ã„ã‚‹ã“ã¨ã¨ã€ç©ã¿é‡ã­ã¦ããŸã“ã¨ã‚’ã€çµã³ç›´ã™ã€‚
</p>
""", unsafe_allow_html=True)

APP_STATE_KEYS = [
    "job_text_snapshot",
    "resume_text_snapshot",
    "requirements_raw",
    "requirements_selected",
    "gaps",
    "questions",
    "need_review_ids",
    "dialog_by_requirement",
    "dialog_queue",
    "dialog_index",
    "addendum_materials",
    "addendum_selected",       # â˜… æ›¸ãè¶³ã™å¯¾è±¡ã¨ã—ã¦é¸æŠã•ã‚ŒãŸã‚‚ã®
    "addendum_text",
    "current_step",
    "debug_events",
]

for k in APP_STATE_KEYS:
    if k not in st.session_state:
        st.session_state[k] = None

if st.session_state["current_step"] is None:
    st.session_state["current_step"] = 1

def reset_app_state():
    for k in APP_STATE_KEYS:
        st.session_state[k] = None
    st.session_state["current_step"] = 1

def get_requirements_for_use() -> Optional[Dict[str, Any]]:
    return st.session_state.get("requirements_selected") or st.session_state.get("requirements_raw")

# é€²æ—ã‚¹ãƒ†ãƒƒãƒ‘ãƒ¼
st.markdown("---")
render_progress_stepper(st.session_state["current_step"])
st.markdown("---")


# ============================================================
# STEP 1: å…¥åŠ›
# ============================================================

if st.session_state["current_step"] == 1:
    st.header("â‘  æ±‚äººç¥¨ã¨è·å‹™çµŒæ­´æ›¸ã‚’å…¥åŠ›")

    col_job, col_resume = st.columns(2)

    with col_job:
        st.subheader("æ±‚äººç¥¨")
        job_mode = st.radio("å…¥åŠ›æ–¹æ³•", ["ãƒ†ã‚­ã‚¹ãƒˆè²¼ã‚Šä»˜ã‘", "PDFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"], horizontal=True, key="job_mode")

        if job_mode == "ãƒ†ã‚­ã‚¹ãƒˆè²¼ã‚Šä»˜ã‘":
            job_text = st.text_area("æ±‚äººç¥¨ã®å†…å®¹ã‚’ã‚³ãƒ”ãƒ¼ï¼†ãƒšãƒ¼ã‚¹ãƒˆ", height=280, key="job_text_input",
                                    placeholder="æ±‚äººç¥¨ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„...")
        else:
            job_pdf = st.file_uploader("PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ", type=["pdf"], key="job_pdf")
            if job_pdf:
                with st.spinner("èª­ã¿è¾¼ã¿ä¸­..."):
                    job_text = extract_text_from_pdf(job_pdf.getvalue())
                if job_text:
                    job_text = st.text_area("æŠ½å‡ºçµæœï¼ˆå¿…è¦ã«å¿œã˜ã¦ç·¨é›†ï¼‰", value=job_text, height=250, key="job_text_from_pdf")
                else:
                    st.warning("ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚æ‰‹å‹•ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
                    job_text = ""
            else:
                job_text = ""

        job_len = len(job_text) if job_text else 0
        if job_len > 0 and job_len < MIN_INPUT_CHARS:
            st.warning(f"å…¥åŠ›ãŒçŸ­ã„ã§ã™ï¼ˆ{job_len}æ–‡å­—ï¼‰ã€‚æ±‚äººç¥¨å…¨ä½“ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        elif job_len > 0:
            st.caption(f"{job_len}æ–‡å­—")

    with col_resume:
        st.subheader("è·å‹™çµŒæ­´æ›¸")
        resume_mode = st.radio("å…¥åŠ›æ–¹æ³•", ["ãƒ†ã‚­ã‚¹ãƒˆè²¼ã‚Šä»˜ã‘", "PDFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"], horizontal=True, key="resume_mode")

        if resume_mode == "ãƒ†ã‚­ã‚¹ãƒˆè²¼ã‚Šä»˜ã‘":
            resume_text = st.text_area("è·å‹™çµŒæ­´æ›¸ã®å†…å®¹ã‚’ã‚³ãƒ”ãƒ¼ï¼†ãƒšãƒ¼ã‚¹ãƒˆ", height=280, key="resume_text_input",
                                       placeholder="è·å‹™çµŒæ­´æ›¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„...")
        else:
            resume_pdf = st.file_uploader("PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ", type=["pdf"], key="resume_pdf")
            if resume_pdf:
                with st.spinner("èª­ã¿è¾¼ã¿ä¸­..."):
                    resume_text = extract_text_from_pdf(resume_pdf.getvalue())
                if resume_text:
                    resume_text = st.text_area("æŠ½å‡ºçµæœï¼ˆå¿…è¦ã«å¿œã˜ã¦ç·¨é›†ï¼‰", value=resume_text, height=250, key="resume_text_from_pdf")
                else:
                    st.warning("ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚æ‰‹å‹•ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
                    resume_text = ""
            else:
                resume_text = ""

        resume_len = len(resume_text) if resume_text else 0
        if resume_len > 0 and resume_len < MIN_INPUT_CHARS:
            st.warning(f"å…¥åŠ›ãŒçŸ­ã„ã§ã™ï¼ˆ{resume_len}æ–‡å­—ï¼‰ã€‚è·å‹™çµŒæ­´æ›¸å…¨ä½“ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        elif resume_len > 0:
            st.caption(f"{resume_len}æ–‡å­—")

    can_proceed = (job_text and len(job_text) >= MIN_INPUT_CHARS and resume_text and len(resume_text) >= MIN_INPUT_CHARS)

    if st.button("æ¬¡ã¸é€²ã‚€ â†’", use_container_width=True, type="primary", disabled=not can_proceed):
        st.session_state["job_text_snapshot"] = job_text
        st.session_state["resume_text_snapshot"] = resume_text
        st.session_state["current_step"] = 2

        # ä»¥é™ã‚’ãƒªã‚»ãƒƒãƒˆ
        st.session_state["requirements_raw"] = None
        st.session_state["requirements_selected"] = None
        st.session_state["gaps"] = None
        st.session_state["questions"] = None
        st.session_state["need_review_ids"] = None
        st.session_state["dialog_by_requirement"] = None
        st.session_state["dialog_queue"] = None
        st.session_state["dialog_index"] = None
        st.session_state["addendum_materials"] = None
        st.session_state["addendum_selected"] = None
        st.session_state["addendum_text"] = None
        st.rerun()

    if not can_proceed and (job_text or resume_text):
        st.info("æ±‚äººç¥¨ãƒ»è·å‹™çµŒæ­´æ›¸ã®ä¸¡æ–¹ã‚’ååˆ†ãªé•·ã•ã§å…¥åŠ›ã™ã‚‹ã¨æ¬¡ã¸é€²ã‚ã¾ã™ã€‚")

else:
    # å…¥åŠ›å†…å®¹ã®æŠ˜ã‚ŠãŸãŸã¿
    with st.expander("å…¥åŠ›æ¸ˆã¿ã®å†…å®¹ã‚’ç¢ºèª", expanded=False):
        col1, col2 = st.columns(2)
        with col1:
            st.caption("æ±‚äººç¥¨ï¼ˆå…ˆé ­300æ–‡å­—ï¼‰")
            job_preview = (st.session_state.get("job_text_snapshot") or "")[:300]
            st.text(job_preview + ("..." if len(st.session_state.get("job_text_snapshot") or "") > 300 else ""))
        with col2:
            st.caption("è·å‹™çµŒæ­´æ›¸ï¼ˆå…ˆé ­300æ–‡å­—ï¼‰")
            resume_preview = (st.session_state.get("resume_text_snapshot") or "")[:300]
            st.text(resume_preview + ("..." if len(st.session_state.get("resume_text_snapshot") or "") > 300 else ""))


# ============================================================
# STEP 2: ã‚¹ã‚­ãƒ«æ•´ç†ï¼ˆè¦ä»¶æŠ½å‡ºï¼‰
# ============================================================

if st.session_state["current_step"] == 2:
    st.header("â‘¡ æ±‚ã‚ã‚‰ã‚Œã‚‹ã‚¹ã‚­ãƒ«ãƒ»çµŒé¨“ã‚’æ•´ç†")

    if st.session_state.get("requirements_raw") is None:
        st.markdown("æ±‚äººç¥¨ã‹ã‚‰ã€Œæ±‚ã‚ã‚‰ã‚Œã‚‹ã‚¹ã‚­ãƒ«ãƒ»çµŒé¨“ã€ã‚’è‡ªå‹•ã§æŠ½å‡ºã—ã¾ã™ã€‚")

        if st.button("æŠ½å‡ºã‚’é–‹å§‹", use_container_width=True, type="primary"):
            try:
                with st.status("æ±‚äººç¥¨ã‚’åˆ†æã—ã¦ã„ã¾ã™...", expanded=True) as status:
                    st.write("æ±‚äººç¥¨ã®å†…å®¹ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
                    st.write("æ±‚ã‚ã‚‰ã‚Œã‚‹ã‚¹ã‚­ãƒ«ãƒ»çµŒé¨“ã‚’æŠ½å‡ºä¸­...")
                    st.caption("ï¼ˆé€šå¸¸20ã€œ40ç§’ã‹ã‹ã‚Šã¾ã™ï¼‰")

                    req = extract_requirements(st.session_state["job_text_snapshot"])

                    status.update(label="æŠ½å‡ºãŒå®Œäº†ã—ã¾ã—ãŸ", state="complete")

                st.session_state["requirements_raw"] = req
                st.rerun()

            except OpenAIAppError as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            except Exception as e:
                log_debug_event("unexpected_exception_step2", {"error": repr(e)})
                st.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    else:
        req_rows = st.session_state["requirements_raw"].get("requirements", [])
        st.success(f"{len(req_rows)}ä»¶ã®ã‚¹ã‚­ãƒ«ãƒ»çµŒé¨“ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")

        st.markdown("#### è¨ºæ–­ã«å«ã‚ã‚‹é …ç›®ã‚’é¸æŠ")
        st.caption("å¿œå‹Ÿæ–¹æ³•ãªã©è¨ºæ–­ã«ä¸è¦ãªé …ç›®ã¯å¤–ã—ã¦ãã ã•ã„")

        df_req = pd.DataFrame(req_rows)
        if "include" not in df_req.columns:
            df_req["include"] = df_req["text"].apply(default_include_flag)

        df_req = df_req[["include", "id", "text"]].rename(columns={"include": "è¨ºæ–­ã«å«ã‚ã‚‹", "id": "ID", "text": "å†…å®¹"})

        edited = st.data_editor(
            df_req,
            use_container_width=True,
            hide_index=True,
            column_config={
                "è¨ºæ–­ã«å«ã‚ã‚‹": st.column_config.CheckboxColumn("å«ã‚ã‚‹", width="small"),
                "ID": st.column_config.TextColumn("ID", disabled=True, width="small"),
                "å†…å®¹": st.column_config.TextColumn("å†…å®¹", disabled=True),
            },
            key="requirements_editor",
        )

        selected_req = build_selected_requirements_from_editor(edited)
        selected_count = len(selected_req["requirements"])
        st.info(f"é¸æŠä¸­: {selected_count}ä»¶ / å…¨{len(req_rows)}ä»¶")

        col_back, col_next = st.columns([1, 2])
        with col_back:
            if st.button("â† å…¥åŠ›ã«æˆ»ã‚‹", use_container_width=True):
                st.session_state["current_step"] = 1
                st.session_state["requirements_raw"] = None
                st.session_state["requirements_selected"] = None
                st.rerun()

        with col_next:
            if st.button("æ¬¡ã¸é€²ã‚€ â†’", use_container_width=True, type="primary", disabled=(selected_count == 0)):
                st.session_state["requirements_selected"] = selected_req
                st.session_state["current_step"] = 3
                st.session_state["gaps"] = None
                st.session_state["questions"] = None
                st.session_state["need_review_ids"] = None
                st.session_state["dialog_by_requirement"] = None
                st.session_state["dialog_queue"] = None
                st.session_state["dialog_index"] = None
                st.session_state["addendum_materials"] = None
                st.session_state["addendum_selected"] = None
                st.session_state["addendum_text"] = None
                st.rerun()

elif st.session_state["current_step"] > 2:
    req_selected = get_requirements_for_use()
    if req_selected:
        with st.expander(f"æ•´ç†æ¸ˆã¿: {len(req_selected.get('requirements', []))}ä»¶", expanded=False):
            for r in req_selected.get("requirements", []):
                st.markdown(f"- **{r['id']}**: {r['text']}")


# ============================================================
# STEP 3: çµŒæ­´ã¨æ¯”è¼ƒï¼ˆã‚®ãƒ£ãƒƒãƒ—åˆ¤å®š + è³ªå•ï¼‰
# ============================================================

if st.session_state["current_step"] == 3:
    st.header("â‘¢ ã‚ãªãŸã®çµŒæ­´ã¨æ¯”è¼ƒ")

    req_for_gap = get_requirements_for_use()

    if st.session_state.get("gaps") is None:
        st.markdown("è·å‹™çµŒæ­´æ›¸ã¨ç…§ã‚‰ã—åˆã‚ã›ã¦ã€ã‚¢ãƒ”ãƒ¼ãƒ«ã§ãã¦ã„ã‚‹ç‚¹ãƒ»è£œè¶³ãŒå¿…è¦ãªç‚¹ã‚’åˆ¤å®šã—ã¾ã™ã€‚")

        if st.button("æ¯”è¼ƒã‚’é–‹å§‹", use_container_width=True, type="primary"):
            try:
                with st.status("è·å‹™çµŒæ­´æ›¸ã‚’åˆ†æã—ã¦ã„ã¾ã™...", expanded=True) as status:
                    st.write("è·å‹™çµŒæ­´æ›¸ã®å†…å®¹ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
                    st.write("æ±‚ã‚ã‚‰ã‚Œã‚‹ã‚¹ã‚­ãƒ«ã¨ç…§åˆä¸­...")
                    st.caption("ï¼ˆé€šå¸¸30ç§’ã€œ1åˆ†ã‹ã‹ã‚Šã¾ã™ï¼‰")

                    gaps = classify_gaps(req_for_gap, st.session_state["resume_text_snapshot"])

                    status.update(label="æ¯”è¼ƒãŒå®Œäº†ã—ã¾ã—ãŸ", state="complete")

                st.session_state["gaps"] = gaps
                st.session_state["need_review_ids"] = build_need_review_requirement_ids(gaps)
                st.rerun()

            except OpenAIAppError as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            except Exception as e:
                log_debug_event("unexpected_exception_step3_compare", {"error": repr(e)})
                st.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

        if st.button("â† ã‚¹ã‚­ãƒ«æ•´ç†ã«æˆ»ã‚‹", use_container_width=True):
            st.session_state["current_step"] = 2
            st.rerun()

    else:
        st.markdown("#### æ¯”è¼ƒçµæœï¼ˆã‚µãƒãƒªãƒ¼ï¼‰")
        render_gaps_summary(st.session_state["gaps"])
        st.markdown("---")
        st.markdown("#### è©³ç´°")
        render_gaps_detail(st.session_state["gaps"], req_for_gap)

        need_ids = st.session_state.get("need_review_ids") or []
        st.markdown("---")

        if len(need_ids) == 0:
            st.success("è£œè¶³ãŒå¿…è¦ï¼è¨˜è¼‰ãªã—ã®é …ç›®ãŒãªã„ãŸã‚ã€æ›¸ãè¶³ã—ã¯ä¸è¦ãã†ã§ã™ã€‚")
            col_back, col_next = st.columns([1, 2])
            with col_back:
                if st.button("â† ã‚¹ã‚­ãƒ«æ•´ç†ã«æˆ»ã‚‹", use_container_width=True):
                    st.session_state["current_step"] = 2
                    st.rerun()
            with col_next:
                if st.button("â‘¤ã¸é€²ã‚€ï¼ˆæ–‡ç« ä½œæˆï¼‰â†’", use_container_width=True, type="primary"):
                    st.session_state["addendum_materials"] = {}
                    st.session_state["current_step"] = 5
                    st.rerun()
        else:
            st.info(f"ã€Œè£œè¶³ãŒå¿…è¦ã€ã€Œè¨˜è¼‰ãªã—ã€ã®é …ç›®ãŒ {len(need_ids)} ä»¶ã‚ã‚Šã¾ã™ã€‚æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã§å†…å®¹ã‚’æ•´ç†ã—ã¾ã—ã‚‡ã†ã€‚")

            col_back, col_next = st.columns([1, 2])
            with col_back:
                if st.button("â† ã‚¹ã‚­ãƒ«æ•´ç†ã«æˆ»ã‚‹", use_container_width=True):
                    st.session_state["current_step"] = 2
                    st.session_state["gaps"] = None
                    st.session_state["questions"] = None
                    st.session_state["need_review_ids"] = None
                    st.rerun()

            with col_next:
                if st.button("â‘£ã¸é€²ã‚€ï¼ˆå†…å®¹ã‚’æ•´ç†ï¼‰â†’", use_container_width=True, type="primary"):
                    st.session_state["current_step"] = 4
                    st.rerun()

elif st.session_state["current_step"] > 3 and st.session_state.get("gaps"):
    gap_list = st.session_state["gaps"].get("gaps", [])
    counts = {"ã‚¢ãƒ”ãƒ¼ãƒ«æ¸ˆã¿": 0, "è£œè¶³ãŒå¿…è¦": 0, "è¨˜è¼‰ãªã—": 0}
    for g in gap_list:
        s = g.get("status", "")
        if s in counts:
            counts[s] += 1
    summary = f"âœ…{counts['ã‚¢ãƒ”ãƒ¼ãƒ«æ¸ˆã¿']} âš ï¸{counts['è£œè¶³ãŒå¿…è¦']} âŒ{counts['è¨˜è¼‰ãªã—']}"
    with st.expander(f"æ¯”è¼ƒçµæœ: {summary}", expanded=False):
        render_gaps_detail(st.session_state["gaps"], get_requirements_for_use())


# ============================================================
# STEP 4: æ›¸ãè¶³ã™å†…å®¹ã‚’æ•´ç†ï¼ˆå¯¾è©±ï¼‰
# ============================================================

if st.session_state["current_step"] == 4:
    st.header("â‘£ æ›¸ãè¶³ã™å†…å®¹ã‚’æ•´ç†ã™ã‚‹")
    st.caption("è³ªå•ã«ç­”ãˆãªãŒã‚‰ã€è·å‹™çµŒæ­´æ›¸ã«æ›¸ãè¶³ã›ã‚‹çµŒé¨“ãƒ»å®Ÿç¸¾ã‚’æ•´ç†ã—ã¾ã™ã€‚å¯¾è±¡ã¯ã€Œè£œè¶³ãŒå¿…è¦ã€ã€Œè¨˜è¼‰ãªã—ã€ã®é …ç›®ã®ã¿ã§ã™ã€‚")

    req_for_use = get_requirements_for_use()
    req_map = {r["id"]: r["text"] for r in (req_for_use or {}).get("requirements", [])}

    need_ids = st.session_state.get("need_review_ids") or []
    if not need_ids:
        st.info("å¯¾è±¡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚â‘¤ã¸é€²ã¿ã¾ã™ã€‚")
        st.session_state["addendum_materials"] = {}
        st.session_state["current_step"] = 5
        st.rerun()

    # åˆæœŸåŒ–ï¼ˆæœ€åˆã®è¡¨ç¤ºæ™‚ã ã‘ï¼‰
    if st.session_state.get("dialog_queue") is None:
        st.session_state["dialog_queue"] = need_ids
        st.session_state["dialog_index"] = 0
        st.session_state["dialog_by_requirement"] = {}
        st.session_state["addendum_materials"] = {}

    queue: List[str] = st.session_state["dialog_queue"] or []
    idx: int = int(st.session_state.get("dialog_index") or 0)

    # å…¨ã¦å®Œäº†ã—ãŸã‚‰â‘¤ã¸
    if idx >= len(queue):
        st.success("æ›¸ãè¶³ã™å†…å®¹ã®æ•´ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚â‘¤ã¸é€²ã¿ã¾ã™ã€‚")
        st.session_state["current_step"] = 5
        st.rerun()

    current_rid = queue[idx]
    current_req_text = req_map.get(current_rid, "")

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸Šã®å¯¾è©±çŠ¶æ…‹ã‚’å–å¾—
    dialog_state = st.session_state["dialog_by_requirement"].get(current_rid)
    if dialog_state is None:
        dialog_state = {
            "history": [],
            "turns": 0,
            "finalized": False,
            "facts": "",
        }
        st.session_state["dialog_by_requirement"][current_rid] = dialog_state

    # ä¸Šéƒ¨ï¼šå¯¾è±¡è¦ä»¶ã‚«ãƒ¼ãƒ‰
    render_requirement_card(
        current_rid,
        current_req_text or "ï¼ˆè¦ä»¶ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼‰",
        subtitle=f"é€²æ—: {idx+1} / {len(queue)}"
    )

    st.markdown("#### å¯¾è©±")
    st.caption("ã‚³ãƒ„ï¼šæ•°å­—ï¼ˆä»¶æ•°/ç‡/æœŸé–“ï¼‰ã€å½¹å‰²ï¼ˆæ‹…å½“ç¯„å›²ï¼‰ã€æˆæœï¼ˆæ”¹å–„ãƒ»å‰Šæ¸›ãƒ»é”æˆï¼‰ã‚’æ›¸ã‘ã‚‹ã¨å¼·ã„ã§ã™ã€‚")

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´è¡¨ç¤º
    for m in dialog_state["history"]:
        with st.chat_message("assistant" if m["role"] == "assistant" else "user"):
            st.markdown(m["content"])

    # finalizeæ¸ˆã¿ãªã‚‰æ¬¡ã¸
    if dialog_state.get("finalized"):
        st.success("ã“ã®è¦ä»¶ã¯æ•´ç†å®Œäº†ã§ã™ã€‚")
        with st.expander("æ•´ç†ã—ãŸå†…å®¹ï¼ˆç·¨é›†å¯ï¼‰", expanded=False):
            edited_facts = st.text_area(
                "æ•´ç†ã—ãŸå†…å®¹",
                value=dialog_state.get("facts", ""),
                height=140,
                key=f"facts_edit_{current_rid}",
            )
            if isinstance(edited_facts, str):
                dialog_state["facts"] = edited_facts.strip()
                st.session_state["addendum_materials"][current_rid] = dialog_state["facts"]

        col_skip, col_next = st.columns([1, 2])
        with col_skip:
            if st.button("ã“ã®è¦ä»¶ã‚’ã‚„ã‚Šç›´ã™", use_container_width=True):
                st.session_state["dialog_by_requirement"][current_rid] = {
                    "history": [], "turns": 0, "finalized": False, "facts": ""
                }
                st.session_state["addendum_materials"].pop(current_rid, None)
                st.rerun()

        with col_next:
            if st.button("æ¬¡ã®è¦ä»¶ã¸ â†’", use_container_width=True, type="primary"):
                st.session_state["dialog_index"] = idx + 1
                st.rerun()

    else:
        MAX_TURNS_PER_REQ = 4

        # ã¾ã ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã‹ã‚‰ã®æœ€åˆã®å•ã„ãŒãªã„å ´åˆã¯ã€è‡ªå‹•ã§1å›ç”Ÿæˆã—ã¦æç¤º
        if len(dialog_state["history"]) == 0:
            try:
                with st.status("è³ªå•ã‚’æº–å‚™ã—ã¦ã„ã¾ã™...", expanded=False) as status:
                    out = dialog_refine_one_requirement(
                        requirement_id=current_rid,
                        requirement_text=current_req_text,
                        resume_text=st.session_state.get("resume_text_snapshot") or "",
                        chat_history=[],
                    )
                    status.update(label="æº–å‚™å®Œäº†", state="complete")

                dialog_state["history"].append({"role": "assistant", "content": out["next_question"]})

                if out["done"]:
                    facts_list = out.get("facts") or []
                    facts = "\n".join([f"- {f.get('fact', '')}" for f in facts_list if isinstance(f, dict) and f.get('fact')])
                    dialog_state["finalized"] = True
                    dialog_state["facts"] = facts
                    st.session_state["addendum_materials"][current_rid] = facts

                st.rerun()

            except OpenAIAppError as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            except Exception as e:
                log_debug_event("unexpected_exception_step4_init", {"error": repr(e)})
                st.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

        # å…¥åŠ›æ¬„ï¼ˆtext_areaã§é«˜ã•ã‚’ç¢ºä¿ï¼‰
        user_input = st.text_area(
            "ã“ã®è¦ä»¶ã«ã¤ã„ã¦ã€æ€ã„å½“ãŸã‚‹äº‹å®Ÿï¼ˆå®Ÿç¸¾ãƒ»å½¹å‰²ãƒ»æˆæœãªã©ï¼‰ã‚’æ›¸ã„ã¦ãã ã•ã„",
            height=100,
            key=f"dialog_input_{current_rid}_{dialog_state['turns']}",
            placeholder="ä¾‹ï¼š2022å¹´ã‹ã‚‰2å¹´é–“ã€â—‹â—‹ã®ãƒªãƒ¼ãƒ€ãƒ¼ã¨ã—ã¦â–³â–³ã‚’æ‹…å½“ã—ã€â–¡â–¡ã‚’é”æˆã—ã¾ã—ãŸ"
        )
        st.caption("ğŸ’¡ å…¥åŠ›ãŒçµ‚ã‚ã£ãŸã‚‰ã€Œé€ä¿¡ã€ãƒœã‚¿ãƒ³ã€ã¾ãŸã¯ Ctrl+Enter ã§é€ä¿¡ã§ãã¾ã™")
        
        col_submit, col_skip, col_back = st.columns([2, 1, 1])
        with col_submit:
            submit_clicked = st.button("é€ä¿¡ â†’", use_container_width=True, type="primary", disabled=not user_input.strip())
        with col_skip:
            skip_clicked = st.button("ã‚¹ã‚­ãƒƒãƒ—", use_container_width=True)
        with col_back:
            back_clicked = st.button("â† æˆ»ã‚‹", use_container_width=True)

        if back_clicked:
            st.session_state["current_step"] = 3
            st.rerun()
        
        if skip_clicked:
            dialog_state["finalized"] = True
            dialog_state["facts"] = ""
            st.session_state["addendum_materials"][current_rid] = ""
            st.session_state["dialog_index"] = idx + 1
            st.rerun()

        if submit_clicked and user_input.strip():
            dialog_state["history"].append({"role": "user", "content": user_input})
            dialog_state["turns"] = int(dialog_state.get("turns") or 0) + 1

            try:
                with st.status("æ•´ç†ã—ã¦ã„ã¾ã™...", expanded=False) as status:
                    out = dialog_refine_one_requirement(
                        requirement_id=current_rid,
                        requirement_text=current_req_text,
                        resume_text=st.session_state.get("resume_text_snapshot") or "",
                        chat_history=dialog_state["history"],
                    )
                    status.update(label="æ›´æ–°ã—ã¾ã—ãŸ", state="complete")

                dialog_state["history"].append({"role": "assistant", "content": out["next_question"]})

                if out["done"]:
                    facts_list = out.get("facts") or []
                    facts = "\n".join([f"- {f.get('fact', '')}" for f in facts_list if isinstance(f, dict) and f.get('fact')])
                    dialog_state["finalized"] = True
                    dialog_state["facts"] = facts
                    st.session_state["addendum_materials"][current_rid] = facts

                else:
                    if dialog_state["turns"] >= MAX_TURNS_PER_REQ:
                        dialog_state["finalized"] = True
                        user_lines = [m["content"] for m in dialog_state["history"] if m["role"] == "user"]
                        facts = "\n".join([f"- {line.strip()}" for line in user_lines if isinstance(line, str) and line.strip()])
                        dialog_state["facts"] = facts
                        st.session_state["addendum_materials"][current_rid] = facts

                st.rerun()

            except OpenAIAppError as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            except Exception as e:
                log_debug_event("unexpected_exception_step4_turn", {"error": repr(e)})
                st.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


# ============================================================
# STEP 5: æ›¸ãè¶³ã™æ–‡ç« ã‚’ä½œæˆï¼ˆæå‡ºç”¨ï¼‰
# ============================================================

if st.session_state["current_step"] == 5:
    st.header("â‘¤ è·å‹™çµŒæ­´æ›¸ã«æ›¸ãè¶³ã™æ–‡ç« ã‚’ä½œæˆ")

    req_for_addendum = get_requirements_for_use()
    materials: Dict[str, str] = st.session_state.get("addendum_materials") or {}
    req_map = {r["id"]: r["text"] for r in (req_for_addendum or {}).get("requirements", [])}

    # ç©ºç´ æã¯è½ã¨ã™
    cleaned_materials = {k: v.strip() for k, v in materials.items() if isinstance(v, str) and v.strip()}

    if not cleaned_materials:
        st.info("æ›¸ãè¶³ã™å†…å®¹ãŒå…¥åŠ›ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€æ–‡ç« ã¯ä½œæˆã—ã¾ã›ã‚“ã€‚")
        col_back, col_reset = st.columns([1, 1])
        with col_back:
            if st.button("â† å†…å®¹æ•´ç†ã«æˆ»ã‚‹", use_container_width=True):
                st.session_state["current_step"] = 4
                st.rerun()
        with col_reset:
            if st.button("æœ€åˆã‹ã‚‰ã‚„ã‚Šç›´ã™", use_container_width=True):
                reset_app_state()
                st.rerun()
    else:
        # â˜… æ›¸ãè¶³ã™å¯¾è±¡ã®ç¢ºèªUIï¼ˆãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ï¼‰
        st.markdown("#### æ›¸ãè¶³ã™å¯¾è±¡ã‚’ç¢ºèª")
        st.caption("ãƒã‚§ãƒƒã‚¯ã‚’å¤–ã—ãŸé …ç›®ã¯ã€æœ€çµ‚æ–‡ç« ã«å«ã¾ã‚Œã¾ã›ã‚“ã€‚")

        # åˆå›ã¯å…¨ã¦é¸æŠçŠ¶æ…‹
        if st.session_state.get("addendum_selected") is None:
            st.session_state["addendum_selected"] = {rid: True for rid in cleaned_materials.keys()}

        selected_state: Dict[str, bool] = st.session_state["addendum_selected"]

        for rid, facts in cleaned_materials.items():
            req_text = req_map.get(rid, "ï¼ˆä¸æ˜ï¼‰")
            col_check, col_content = st.columns([0.08, 0.92])
            with col_check:
                is_selected = st.checkbox(
                    "",
                    value=selected_state.get(rid, True),
                    key=f"select_{rid}",
                    label_visibility="collapsed",
                )
                selected_state[rid] = is_selected
            with col_content:
                bg_color = "#f0f8ff" if is_selected else "#f5f5f5"
                text_color = "#333" if is_selected else "#999"
                st.markdown(f"""
                <div style="background-color:{bg_color}; padding:12px; border-radius:8px; margin-bottom:8px; border-left:4px solid {'#007bff' if is_selected else '#ccc'};">
                    <div style="font-weight:600; margin-bottom:6px; color:{text_color};">ğŸ“Œ {rid}: {req_text[:60]}{'...' if len(req_text) > 60 else ''}</div>
                    <div style="font-size:0.9em; color:{text_color}; white-space:pre-wrap;">{facts[:200]}{'...' if len(facts) > 200 else ''}</div>
                </div>
                """, unsafe_allow_html=True)

        st.session_state["addendum_selected"] = selected_state

        # é¸æŠã•ã‚ŒãŸé …ç›®ã ã‘æŠ½å‡º
        final_materials = {rid: facts for rid, facts in cleaned_materials.items() if selected_state.get(rid, False)}
        selected_count = len(final_materials)

        st.info(f"é¸æŠä¸­: {selected_count}ä»¶ / å…¨{len(cleaned_materials)}ä»¶")

        st.markdown("---")

        # é¸æŠã•ã‚ŒãŸé …ç›®ã®å†…å®¹ç·¨é›†
        if final_materials:
            st.markdown("#### æ›¸ãè¶³ã™å†…å®¹ï¼ˆå¿…è¦ãªã‚‰ç·¨é›†ï¼‰")
            st.caption("ã“ã“ã‚’ç›´ã™ã¨ã€æœ€çµ‚æ–‡ç« ã«ã‚‚åæ˜ ã•ã‚Œã¾ã™ã€‚")

            edited_answers: Dict[str, str] = {}
            for rid, facts in final_materials.items():
                render_requirement_card(rid, req_map.get(rid, ""), subtitle="ã“ã®è¦ä»¶ã«å¯¾ã—ã¦æ•´ç†ã—ãŸå†…å®¹")
                edited_answers[rid] = st.text_area(
                    f"å†…å®¹ï¼ˆ{rid}ï¼‰",
                    value=facts,
                    height=140,
                    key=f"final_material_{rid}",
                    label_visibility="collapsed",
                ).strip()
        else:
            edited_answers = {}

        st.markdown("---")
        col_back, col_generate = st.columns([1, 2])
        with col_back:
            if st.button("â† å†…å®¹æ•´ç†ã«æˆ»ã‚‹", use_container_width=True):
                st.session_state["addendum_materials"] = {**cleaned_materials, **edited_answers}
                st.session_state["current_step"] = 4
                st.rerun()

        with col_generate:
            can_generate = bool(edited_answers) and any(v for v in edited_answers.values())
            if st.button("æ–‡ç« ã‚’ä½œæˆã™ã‚‹", use_container_width=True, type="primary", disabled=not can_generate):
                try:
                    final_answers = {k: v for k, v in edited_answers.items() if isinstance(v, str) and v.strip()}

                    # é¸æŠã•ã‚ŒãŸè¦ä»¶ã®ã¿ã‚’æ¸¡ã™
                    selected_req_ids = set(final_answers.keys())
                    filtered_requirements = {
                        "requirements": [
                            r for r in (req_for_addendum or {}).get("requirements", [])
                            if r.get("id") in selected_req_ids
                        ]
                    }

                    with st.status("æ–‡ç« ã‚’ä½œæˆã—ã¦ã„ã¾ã™...", expanded=True) as status:
                        st.write("æ•´ç†ã—ãŸå†…å®¹ã‚’åæ˜ ã—ã¦æ–‡ç« åŒ–ã—ã¦ã„ã¾ã™...")
                        st.caption("ï¼ˆé€šå¸¸30ç§’ã€œ1åˆ†ã‹ã‹ã‚Šã¾ã™ï¼‰")

                        result = write_addendum(
                            resume_text=st.session_state.get("resume_text_snapshot") or "",
                            requirements=filtered_requirements,
                            answers=final_answers,
                        )

                        status.update(label="æ–‡ç« ãŒå®Œæˆã—ã¾ã—ãŸ", state="complete")

                    st.session_state["addendum_text"] = result
                    st.rerun()

                except OpenAIAppError as e:
                    st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                except Exception as e:
                    log_debug_event("unexpected_exception_step5_generate", {"error": repr(e)})
                    st.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

        # è¡¨ç¤º
        if st.session_state.get("addendum_text"):
            st.markdown("---")
            st.subheader("å®Œæˆã—ãŸæ–‡ç« ")
            st.caption("å†…å®¹ã‚’ç¢ºèªã—ã€å¿…è¦ã«å¿œã˜ã¦ç·¨é›†ã—ã¦ãã ã•ã„ã€‚ã“ã®ã¾ã¾ã‚³ãƒ”ãƒ¼ã—ã¦è·å‹™çµŒæ­´æ›¸ã«è²¼ã‚Šä»˜ã‘ã‚‰ã‚Œã¾ã™ã€‚")

            edited_result = st.text_area(
                "å®Œæˆã—ãŸæ–‡ç« ï¼ˆç·¨é›†ãƒ»ã‚³ãƒ”ãƒ¼å¯ï¼‰",
                st.session_state["addendum_text"],
                height=300,
                key="final_addendum_edit",
                label_visibility="collapsed",
            )

            # ã‚³ãƒ”ãƒ¼ãƒœã‚¿ãƒ³ã®ä»£ã‚ã‚Šã«ãƒ’ãƒ³ãƒˆã‚’è¡¨ç¤º
            st.info("ğŸ’¡ ä¸Šã®ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã‚’é¸æŠã—ã¦ Ctrl+A â†’ Ctrl+C ã§ã‚³ãƒ”ãƒ¼ã§ãã¾ã™")

            # é”æˆæ„Ÿã‚’æ¼”å‡ºï¼ˆæ•°å­—ãƒ™ãƒ¼ã‚¹ã®ã‚µãƒãƒªãƒ¼ï¼‰
            st.markdown("---")
            st.subheader("è¨ºæ–­å®Œäº†")
            
            # çµ±è¨ˆã‚’è¨ˆç®—
            req_count = len((get_requirements_for_use() or {}).get("requirements", []))
            materials_count = len([v for v in (st.session_state.get("addendum_materials") or {}).values() if v])
            
            st.markdown(f"""
            âœ… æ±‚äººç¥¨ã‹ã‚‰ **{req_count}** ä»¶ã®ã‚¹ã‚­ãƒ«è¦ä»¶ã‚’æŠ½å‡º  
            âœ… ã†ã¡ **{materials_count}** ä»¶ã«ã¤ã„ã¦å†…å®¹ã‚’æ•´ç†  
            âœ… æ›¸ãè¶³ã™æ–‡ç« ã‚’ä½œæˆ
            """)
            
            st.caption("ã“ã®æ–‡ç« ã‚’è·å‹™çµŒæ­´æ›¸ã«è¿½åŠ ã—ã¦ã€å¿œå‹Ÿæº–å‚™ã‚’é€²ã‚ã¦ãã ã•ã„ã€‚")

            st.markdown("---")
            if st.button("æœ€åˆã‹ã‚‰ã‚„ã‚Šç›´ã™", use_container_width=True):
                reset_app_state()
                st.rerun()


# ============================================================
# ã‚µã‚¤ãƒ‰ãƒãƒ¼
# ============================================================

with st.sidebar:
    st.markdown("### é€²æ—çŠ¶æ³")

    step_status = {
        "â‘  å…¥åŠ›": "âœ…" if st.session_state.get("job_text_snapshot") else "â€”",
        "â‘¡ ã‚¹ã‚­ãƒ«æ•´ç†": "âœ…" if st.session_state.get("requirements_selected") else "â€”",
        "â‘¢ çµŒæ­´ã¨æ¯”è¼ƒ": "âœ…" if st.session_state.get("gaps") else "â€”",
        "â‘£ å†…å®¹ã‚’æ•´ç†": "âœ…" if (st.session_state.get("addendum_materials") and any(v for v in (st.session_state.get("addendum_materials") or {}).values())) else "â€”",
        "â‘¤ æ–‡ç« ã‚’ä½œæˆ": "âœ…" if st.session_state.get("addendum_text") else "â€”",
    }
    for step, status in step_status.items():
        st.markdown(f"{status} {step}")

    st.markdown("---")
    if st.button("æœ€åˆã‹ã‚‰ã‚„ã‚Šç›´ã™", use_container_width=True):
        reset_app_state()
        st.rerun()

    st.markdown("---")
    st.caption("ãƒ’ãƒ³ãƒˆ")
    st.caption("ãƒ»æ±‚äººç¥¨ãƒ»çµŒæ­´æ›¸ã¯å…¨æ–‡ã‚’å…¥åŠ›ã™ã‚‹ã¨ç²¾åº¦ãŒä¸ŠãŒã‚Šã¾ã™")
    st.caption("ãƒ»æ•°å­—/æœŸé–“/å½¹å‰²/æˆæœãŒã‚ã‚‹ã¨å¼·ã„ã§ã™")
